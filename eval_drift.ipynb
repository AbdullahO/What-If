{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import namedtuple\n",
    "from math import ceil\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from algorithms.snn import SNN\n",
    "from algorithms.snn_biclustering import SNNBiclustering\n",
    "from algorithms.fill_tensor_ALS import ALS\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from synthetic_data_generation.generate_eval import (\n",
    "    sales_data_staggering_assignment,\n",
    "    sales_data_si_assignment,\n",
    "    sales_data_random_assignment,\n",
    "    get_sales_data,\n",
    ")\n",
    "\n",
    "Metric = namedtuple(\n",
    "    \"Metric\",\n",
    "    \"train_time  query_time r2 mse number_estimated_entries number_estimated_feasible_entries\",\n",
    ")\n",
    "ALG_REGISTRY = {\"SNN\": SNN, \"SNNBiclustering\": SNNBiclustering, \"ALS\": ALS}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_partial(\n",
    "    data_gen,\n",
    "    data_assignment,\n",
    "    algorithm,\n",
    "    repeat,\n",
    "    datasize,\n",
    "    chunk_size,\n",
    "    tensor_nan,\n",
    "    init_points=100,\n",
    "):\n",
    "    # generate data\n",
    "    train_time = np.zeros([repeat])\n",
    "    query_time = np.zeros([repeat])\n",
    "    r2 = np.zeros([repeat])\n",
    "    mse = np.zeros([repeat])\n",
    "    number_estimated_entries = np.zeros([repeat])\n",
    "    number_estimated_feasible_entries = np.zeros([repeat])\n",
    "    for i in range(repeat):\n",
    "        data = data_gen(seed=i, N=100, T=datasize)\n",
    "        if i != 0:\n",
    "            if algorithm == \"SNN\":\n",
    "                model._get_anchors.cache.clear()\n",
    "                model._get_beta.cache.clear()\n",
    "            elif algorithm == \"SNNBiclustering\":\n",
    "                model._map_missing_value.cache.clear()\n",
    "                model._get_beta_from_factors.cache.clear()\n",
    "        model = ALG_REGISTRY[algorithm](verbose=False)\n",
    "        no_batches = ceil((datasize) / chunk_size)\n",
    "        start = 0\n",
    "        for batch in range(no_batches):\n",
    "            # if batch ==0:\n",
    "            # end = init_points -1\n",
    "            end = min(start + chunk_size - 1, datasize - 1)\n",
    "            batch_tensor, full_df = data.generate([start, end])\n",
    "            periods = data_assignment(data, seed=i * (batch + 1), T=end - start + 1)\n",
    "            # else:\n",
    "            #     end = min(start + chunk_size - 1, datasize - 1)\n",
    "            #     batch_tensor, full_df = data.generate([start, end])\n",
    "            #     periods = data_assignment(data, seed=i * (batch + 1), T=end - start + 1)\n",
    "\n",
    "            ss_tensor, df_batch = data.auto_subsample(periods, batch_tensor, full_df)\n",
    "            batch_mask = data.mask\n",
    "\n",
    "            batch_mask = batch_mask.astype(bool)\n",
    "            t = time.perf_counter()\n",
    "            if batch == 0:\n",
    "                mask = batch_mask.copy()\n",
    "                tensor = batch_tensor.copy()\n",
    "                model.fit(\n",
    "                    df=df_batch,\n",
    "                    unit_column=\"unit_id\",\n",
    "                    time_column=\"time\",\n",
    "                    metrics=[\"sales\"],\n",
    "                    actions=[\"ads\"],\n",
    "                )\n",
    "                train_time[i] = time.perf_counter() - t\n",
    "            else:\n",
    "                mask = np.concatenate([mask, batch_mask], axis=1)\n",
    "                tensor = np.concatenate([tensor, batch_tensor], axis=1)\n",
    "                model.partial_fit(df_batch)\n",
    "                train_time[i] = time.perf_counter() - t\n",
    "            t = time.perf_counter()\n",
    "            model.query(\n",
    "                [0],\n",
    "                [\"2020-01-01\", \" 2020-01-02\"],\n",
    "                \"sales\",\n",
    "                \"ad 0\",\n",
    "                [\"2020-01-01\", \" 2020-01-02\"],\n",
    "            )\n",
    "            query_time[i] = time.perf_counter() - t\n",
    "            start = end + 1\n",
    "\n",
    "        # adjust tensor\n",
    "        indices = [model.actions_dict[action] for action in [\"ad 0\", \"ad 1\", \"ad 2\"]]\n",
    "        _tensor_est = model.get_tensor_from_factors()\n",
    "        tensor_est = _tensor_est[:, :, indices]\n",
    "        if tensor_nan is not None:\n",
    "            tensor_est[tensor_nan] = np.nan\n",
    "        # accuracy\n",
    "        tensor = tensor[:, :-1, :]\n",
    "        mask = mask[:, :-1, :]\n",
    "        tensor_est = tensor_est[:, :-1, :]\n",
    "        notnan = ~np.isnan(tensor_est)\n",
    "        r2[i] = r2_score(\n",
    "            tensor[notnan, 0][~mask[notnan, 0]].flatten(),\n",
    "            tensor_est[notnan][~mask[notnan, 0]].flatten(),\n",
    "        )\n",
    "        print(r2[i])\n",
    "        mse[i] = np.nanmean(\n",
    "            np.square(\n",
    "                tensor[..., 0][~mask[..., 0]].flatten()\n",
    "                - tensor_est[:][~mask[..., 0]].flatten()\n",
    "            )\n",
    "        )\n",
    "        number_estimated_entries[i] = (~np.isnan(tensor_est[:][~mask[..., 0]])).sum()\n",
    "        number_estimated_feasible_entries[i] = np.nansum(model.feasible)\n",
    "\n",
    "    return Metric(\n",
    "        train_time,\n",
    "        query_time,\n",
    "        r2,\n",
    "        mse,\n",
    "        number_estimated_entries,\n",
    "        number_estimated_feasible_entries,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# data examples\n",
    "datasets_assignment_generators = [\n",
    "    sales_data_si_assignment,\n",
    "    sales_data_staggering_assignment,\n",
    "    sales_data_random_assignment,\n",
    "]\n",
    "data_names = [\"SI sparsity\", \"staggered\", \"random\"]\n",
    "num_datasets = len(datasets_assignment_generators)\n",
    "\n",
    "algorithms = [\"ALS\", \"SNNBiclustering\"]\n",
    "chunksize = [100]\n",
    "datasize = 1000\n",
    "for k, data_assignment in enumerate(datasets_assignment_generators[:]):\n",
    "    for alg in algorithms:\n",
    "            for chunk_size in chunksize:\n",
    "                ## Temp solution for mask problem\n",
    "                r2_scores = evaluate_partial(\n",
    "                    data_gen,\n",
    "                    data_assignment,\n",
    "                    alg,\n",
    "                    datasize,\n",
    "                    chunk_size,\n",
    "                )\n",
    "                print(f\"Evaluate {alg} for {data_names[k]}\")\n",
    "                print(f\"Train time: \\t {train_time[k,:].mean()}\")\n",
    "                print(f\"query time: \\t {query_time[k,:].mean()}\")\n",
    "                print(f\"R2: \\t {r2[k,:].mean()}\")\n",
    "                print(f\"RMSE: \\t {np.sqrt(mse)[k,:].mean()}\")\n",
    "                print(\n",
    "                    f\"number of retrieved entries: \\t {number_estimated_entries[k,:].mean()}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"number of feasible retrieved entries: \\t {number_estimated_feasible_entries[k,:].mean()}\"\n",
    "                )\n",
    "                print(\"=\" * 100)\n",
    "                res_df.loc[res_df.shape[0]] = [\n",
    "                    args.datasize,\n",
    "                    chunk_size,\n",
    "                    alg,\n",
    "                    data_names[k],\n",
    "                    train_time[k, :].mean(),\n",
    "                    query_time[k, :].mean(),\n",
    "                    r2[k, :].mean(),\n",
    "                    np.sqrt(mse)[k, :].mean(),\n",
    "                    number_estimated_entries[k, :].mean(),\n",
    "                ]\n",
    "                res_df.to_csv(\"test_metrics.csv\")\n",
    "\n",
    "    print(\"summary:\")\n",
    "    print(res_df)\n",
    "\n",
    "    if args.export:\n",
    "        res_df.to_csv(\"test_metrics.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('whatIf-Env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4c514d80b85c8cd25b309d75abc24098996b61fa42c8df78da8164087d150b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
